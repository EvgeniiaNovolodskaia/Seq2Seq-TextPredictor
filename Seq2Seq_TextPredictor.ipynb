{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZgKHfkBwh2LA"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvgeniiaNovolodskaia/Seq2Seq-TextPredictor/blob/main/Seq2Seq_TextPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An16FNHuhZI1"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "\n",
        "We have to pre-process the dataset a little bit in order to remove everything that is not part of the text and to split the actual text into paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_toy_dataset = False # If True, a toy dataset (see below) is used, instead of the \"real\" one."
      ],
      "metadata": {
        "id": "veOPiPCIlOOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VXDpa1tiSfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f63fc53-758a-422e-87c9-35b3fceb3edb"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")\n",
        "filename = tmp[0]\n",
        "print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpufml613e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4hirpsiWX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95efb02c-48f2-43de-9115-864ff496290a"
      },
      "source": [
        "# Example\n",
        "with open(filename) as f:\n",
        "  for i in range(200):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ﻿The Project Gutenberg eBook of Madame Bovary\n",
            "[1]     \n",
            "[2] This ebook is for the use of anyone anywhere in the United States and\n",
            "[3] most other parts of the world at no cost and with almost no restrictions\n",
            "[4] whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "[5] of the Project Gutenberg License included with this ebook or online\n",
            "[6] at www.gutenberg.org. If you are not located in the United States,\n",
            "[7] you will have to check the laws of the country where you are located\n",
            "[8] before using this eBook.\n",
            "[9] \n",
            "[10] Title: Madame Bovary\n",
            "[11] \n",
            "[12] Author: Gustave Flaubert\n",
            "[13] \n",
            "[14] Release date: November 26, 2004 [eBook #14155]\n",
            "[15]                 Most recently updated: December 18, 2020\n",
            "[16] \n",
            "[17] Language: French\n",
            "[18] \n",
            "[19] Credits: Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[20] \n",
            "[21] \n",
            "[22] *** START OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY ***\n",
            "[23] \n",
            "[24] \n",
            "[25] \n",
            "[26] \n",
            "[27] Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[28] \n",
            "[29] \n",
            "[30] \n",
            "[31] \n",
            "[32] \n",
            "[33] Gustave Flaubert\n",
            "[34] MADAME BOVARY\n",
            "[35] \n",
            "[36] \n",
            "[37] (1857)\n",
            "[38] \n",
            "[39] \n",
            "[40] Table des matières\n",
            "[41] \n",
            "[42] PREMIÈRE PARTIE\n",
            "[43] I\n",
            "[44] II\n",
            "[45] III\n",
            "[46] IV\n",
            "[47] V\n",
            "[48] VI\n",
            "[49] VII\n",
            "[50] VIII\n",
            "[51] IX\n",
            "[52] DEUXIÈME PARTIE\n",
            "[53] I\n",
            "[54] II\n",
            "[55] III\n",
            "[56] IV\n",
            "[57] V\n",
            "[58] VI\n",
            "[59] VII\n",
            "[60] VIII\n",
            "[61] IX\n",
            "[62] X\n",
            "[63] XI\n",
            "[64] XII\n",
            "[65] XIII\n",
            "[66] XIV\n",
            "[67] XV\n",
            "[68] TROISIÈME PARTIE\n",
            "[69] I\n",
            "[70] II\n",
            "[71] III\n",
            "[72] IV\n",
            "[73] V\n",
            "[74] VI\n",
            "[75] VII\n",
            "[76] VIII\n",
            "[77] IX\n",
            "[78] X\n",
            "[79] XI\n",
            "[80] \n",
            "[81] \n",
            "[82] À Marie-Antoine-Jules Senard\n",
            "[83] \n",
            "[84] MEMBRE DU BARREAU DE PARIS EX-PRESIDENT DE L'ASSEMBLÉE NATIONALE\n",
            "[85] ET ANCIEN MINISTRE DE L'INTÉRIEUR\n",
            "[86] \n",
            "[87] Cher et illustre ami,\n",
            "[88] \n",
            "[89] Permettez-moi d'inscrire votre nom en tête de ce livre et au-\n",
            "[90] dessus même de sa dédicace; car c'est à vous, surtout, que j'en\n",
            "[91] dois la publication. En passant par votre magnifique plaidoirie,\n",
            "[92] mon oeuvre a acquis pour moi-même comme une autorité imprévue.\n",
            "[93] Acceptez donc ici l'hommage de ma gratitude, qui, si grande\n",
            "[94] qu'elle puisse être, ne sera jamais à la hauteur de votre\n",
            "[95] éloquence et de votre dévouement.\n",
            "[96] \n",
            "[97] GUSTAVE FLAUBERT\n",
            "[98] \n",
            "[99] Paris, 12 avril 1857\n",
            "[100] \n",
            "[101] \n",
            "[102] À Louis Bouilhet\n",
            "[103] \n",
            "[104] \n",
            "[105] PREMIÈRE PARTIE\n",
            "[106] \n",
            "[107] \n",
            "[108] I\n",
            "[109] \n",
            "[110] Nous étions à l'Étude, quand le Proviseur entra, suivi d'un\n",
            "[111] nouveau habillé en bourgeois et d'un garçon de classe qui portait\n",
            "[112] un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se\n",
            "[113] leva comme surpris dans son travail.\n",
            "[114] \n",
            "[115] Le Proviseur nous fit signe de nous rasseoir; puis, se tournant\n",
            "[116] vers le maître d'études:\n",
            "[117] \n",
            "[118] -- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je\n",
            "[119] vous recommande, il entre en cinquième. Si son travail et sa\n",
            "[120] conduite sont méritoires, il passera dans les grands, où l'appelle\n",
            "[121] son âge.\n",
            "[122] \n",
            "[123] Resté dans l'angle, derrière la porte, si bien qu'on l'apercevait\n",
            "[124] à peine, le nouveau était un gars de la campagne, d'une quinzaine\n",
            "[125] d'années environ, et plus haut de taille qu'aucun de nous tous. Il\n",
            "[126] avait les cheveux coupés droit sur le front, comme un chantre de\n",
            "[127] village, l'air raisonnable et fort embarrassé. Quoiqu'il ne fût\n",
            "[128] pas large des épaules, son habit-veste de drap vert à boutons\n",
            "[129] noirs devait le gêner aux entournures et laissait voir, par la\n",
            "[130] fente des parements, des poignets rouges habitués à être nus. Ses\n",
            "[131] jambes, en bas bleus, sortaient d'un pantalon jaunâtre très tiré\n",
            "[132] par les bretelles. Il était chaussé de souliers forts, mal cirés,\n",
            "[133] garnis de clous.\n",
            "[134] \n",
            "[135] On commença la récitation des leçons. Il les écouta de toutes ses\n",
            "[136] oreilles, attentif comme au sermon, n'osant même croiser les\n",
            "[137] cuisses, ni s'appuyer sur le coude, et, à deux heures, quand la\n",
            "[138] cloche sonna, le maître d'études fut obligé de l'avertir, pour\n",
            "[139] qu'il se mît avec nous dans les rangs.\n",
            "[140] \n",
            "[141] Nous avions l'habitude, en entrant en classe, de jeter nos\n",
            "[142] casquettes par terre, afin d'avoir ensuite nos mains plus libres;\n",
            "[143] il fallait, dès le seuil de la porte, les lancer sous le banc, de\n",
            "[144] façon à frapper contre la muraille en faisant beaucoup de\n",
            "[145] poussière; c'était là le genre.\n",
            "[146] \n",
            "[147] Mais, soit qu'il n'eût pas remarqué cette manoeuvre ou qu'il n'eut\n",
            "[148] osé s'y soumettre, la prière était finie que le nouveau tenait\n",
            "[149] encore sa casquette sur ses deux genoux. C'était une de ces\n",
            "[150] coiffures d'ordre composite, où l'on retrouve les éléments du\n",
            "[151] bonnet à poil, du chapska, du chapeau rond, de la casquette de\n",
            "[152] loutre et du bonnet de coton, une de ces pauvres choses, enfin,\n",
            "[153] dont la laideur muette a des profondeurs d'expression comme le\n",
            "[154] visage d'un imbécile. Ovoïde et renflée de baleines, elle\n",
            "[155] commençait par trois boudins circulaires; puis s'alternaient,\n",
            "[156] séparés par une bande rouge, des losanges de velours et de poils\n",
            "[157] de lapin; venait ensuite une façon de sac qui se terminait par un\n",
            "[158] polygone cartonné, couvert d'une broderie en soutache compliquée,\n",
            "[159] et d'où pendait, au bout d'un long cordon trop mince, un petit\n",
            "[160] croisillon de fils d'or, en manière de gland. Elle était neuve; la\n",
            "[161] visière brillait.\n",
            "[162] \n",
            "[163] -- Levez-vous, dit le professeur.\n",
            "[164] \n",
            "[165] Il se leva; sa casquette tomba. Toute la classe se mit à rire.\n",
            "[166] \n",
            "[167] Il se baissa pour la reprendre. Un voisin la fit tomber d'un coup\n",
            "[168] de coude, il la ramassa encore une fois.\n",
            "[169] \n",
            "[170] -- Débarrassez-vous donc de votre casque, dit le professeur, qui\n",
            "[171] était un homme d'esprit.\n",
            "[172] \n",
            "[173] Il y eut un rire éclatant des écoliers qui décontenança le pauvre\n",
            "[174] garçon, si bien qu'il ne savait s'il fallait garder sa casquette à\n",
            "[175] la main, la laisser par terre ou la mettre sur sa tête. Il se\n",
            "[176] rassit et la posa sur ses genoux.\n",
            "[177] \n",
            "[178] -- Levez-vous, reprit le professeur, et dites-moi votre nom.\n",
            "[179] \n",
            "[180] Le nouveau articula, d'une voix bredouillante, un nom\n",
            "[181] inintelligible.\n",
            "[182] \n",
            "[183] -- Répétez!\n",
            "[184] \n",
            "[185] Le même bredouillement de syllabes se fit entendre, couvert par\n",
            "[186] les huées de la classe.\n",
            "[187] \n",
            "[188] -- Plus haut! cria le maître, plus haut!\n",
            "[189] \n",
            "[190] Le nouveau, prenant alors une résolution extrême, ouvrit une\n",
            "[191] bouche démesurée et lança à pleins poumons, comme pour appeler\n",
            "[192] quelqu'un, ce mot: Charbovari.\n",
            "[193] \n",
            "[194] Ce fut un vacarme qui s'élança d'un bond, monta en crescendo, avec\n",
            "[195] des éclats de voix aigus (on hurlait, on aboyait, on trépignait,\n",
            "[196] on répétait: Charbovari! Charbovari!), puis qui roula en notes\n",
            "[197] isolées, se calmant à grand-peine, et parfois qui reprenait tout à\n",
            "[198] coup sur la ligne d'un banc où saillissait encore çà et là, comme\n",
            "[199] un pétard mal éteint, quelque rire étouffé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctl4Z9Gti-6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504ce1e6-7166-49b7-9c60-9b83c05e42c9"
      },
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "\n",
        "with open(filename) as f:\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"PREMIÈRE PARTIE\"): skip -= 1;\n",
        "\n",
        "  paragraphs = []\n",
        "  paragraph_buffer = []\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(\"END OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY\" in line): break # End of the actual text.\n",
        "\n",
        "    if(line == \"\"):\n",
        "      if(len(paragraph_buffer) > 0):\n",
        "        paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs.append(paragraph)\n",
        "        paragraph_buffer = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')): paragraph_buffer.append(' ')\n",
        "    paragraph_buffer.append(line)\n",
        "\n",
        "print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "for i in range(3): print(paragraphs[i], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2995 paragraphs read.\n",
            "Nous étions à l'Étude, quand le Proviseur entra, suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se leva comme surpris dans son travail.\n",
            "Le Proviseur nous fit signe de nous rasseoir; puis, se tournant vers le maître d'études:\n",
            "-- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je vous recommande, il entre en cinquième. Si son travail et sa conduite sont méritoires, il passera dans les grands, où l'appelle son âge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy dataset to experiment and firstly learn a model more easily"
      ],
      "metadata": {
        "id": "QZttAP5uj6y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_toy_dataset):\n",
        "  paragraphs = []\n",
        "\n",
        "  import random, string\n",
        "  characters = list(string.ascii_lowercase + string.ascii_lowercase.upper() + \"_-/\\'[]()\")\n",
        "  random.shuffle(characters)\n",
        "  k = random.randint(1, 10)\n",
        "  a = \"a\"\n",
        "  paragraph = (a * k)\n",
        "  for _ in range(100):\n",
        "    random.shuffle(characters)\n",
        "    for a in characters:\n",
        "      k = random.randint(1, 16)\n",
        "      paragraph += f\"? Now, please write {k} {a}.{EOP}\"\n",
        "      paragraphs.append(paragraph)\n",
        "      paragraph = (a * k)\n",
        "  print(f\"{len(paragraphs)} paragraphs generated.\")\n",
        "\n",
        "  print(paragraphs[:10])"
      ],
      "metadata": {
        "id": "28V5KvT6jUFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAn1SqQE1Hr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37320a20-81a8-4c81-abee-b6ad9828f8a8"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "char_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for char in paragraph: char_counts[char] += 1\n",
        "\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\")\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 different characters found in the dataset.\n",
            "[(' ', 109336), ('e', 76369), ('a', 44997), ('s', 42972), ('t', 38893), ('i', 38322), ('n', 35099), ('r', 34314), ('l', 33711), ('u', 32914), ('o', 27415), ('d', 19187), ('c', 14786), ('m', 14638), ('p', 13790), (',', 12378), ('v', 8441), ('é', 8263), (\"'\", 7451), ('.', 6225), ('b', 5519), ('q', 5455), ('f', 5406), ('h', 5386), ('g', 4704), ('-', 4243), ('\\n', 2995), ('à', 2722), ('x', 2057), ('j', 1728), ('è', 1644), ('y', 1619), ('!', 1512), ('E', 1477), (';', 1425), ('ê', 1188), ('L', 981), ('C', 945), ('I', 769), ('M', 743), ('z', 674), ('A', 543), ('?', 530), (':', 480), ('ç', 470), ('B', 427), ('â', 410), ('P', 394), ('î', 327), ('R', 319), ('D', 313), ('O', 301), ('S', 298), ('ô', 296), ('ù', 293), ('H', 270), ('û', 241), ('Q', 237), ('J', 233), ('T', 211), ('V', 181), ('N', 155), ('U', 122), ('«', 120), ('»', 112), ('À', 84), ('F', 84), ('Y', 80), ('_', 64), ('G', 62), ('(', 55), (')', 55), ('ï', 37), ('É', 25), ('k', 16), ('1', 16), ('ë', 9), ('2', 8), ('9', 8), ('3', 8), ('Ç', 7), ('X', 6), ('6', 6), ('5', 5), ('4', 5), ('ü', 5), ('8', 4), ('Ê', 4), ('°', 4), ('W', 3), ('7', 3), ('w', 2), ('0', 1), ('Î', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF0YFZTxD_A0"
      },
      "source": [
        "id_to_char = list(char_counts.keys()) # list of unique characters\n",
        "char_vocabulary = {c: i for i, c in enumerate(id_to_char)} # dictionary mapping characters to ids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71VlK5e3Gg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25635890-098b-439b-cdb4-c6db693baff8"
      },
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "\n",
        "print(char_vocabulary)\n",
        "print(id_to_char)\n",
        "print(f\"EOP_id = {EOP_id}\")\n",
        "\n",
        "# Implementation of the test that proves that implementations of 'char_vocabulary' and 'id_to_char' are consistent.\n",
        "\n",
        "def test_mappings_consistency(char_vocabulary, id_to_char):\n",
        "  for i, char in enumerate(id_to_char):\n",
        "    if char_vocabulary[char] != i:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "print(\"Testing mappings consistency...\")\n",
        "consistency_result = test_mappings_consistency(char_vocabulary, id_to_char)\n",
        "print(f\"Are 'char_vocabulary' and 'id_to_char' consistent? {consistency_result}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'N': 0, 'o': 1, 'u': 2, 's': 3, ' ': 4, 'é': 5, 't': 6, 'i': 7, 'n': 8, 'à': 9, 'l': 10, \"'\": 11, 'É': 12, 'd': 13, 'e': 14, ',': 15, 'q': 16, 'a': 17, 'P': 18, 'r': 19, 'v': 20, 'h': 21, 'b': 22, 'g': 23, 'ç': 24, 'c': 25, 'p': 26, '.': 27, 'C': 28, 'x': 29, 'm': 30, 'è': 31, '\\n': 32, 'L': 33, 'f': 34, ';': 35, 'î': 36, ':': 37, '-': 38, 'M': 39, 'R': 40, 'j': 41, 'S': 42, 'ù': 43, 'â': 44, 'z': 45, 'I': 46, 'Q': 47, 'û': 48, 'ê': 49, 'O': 50, 'y': 51, 'k': 52, 'ï': 53, 'E': 54, 'T': 55, 'U': 56, 'D': 57, '!': 58, '(': 59, ')': 60, 'B': 61, '?': 62, '_': 63, 'G': 64, '1': 65, '8': 66, '2': 67, 'A': 68, 'À': 69, 'H': 70, 'ô': 71, 'V': 72, 'Ê': 73, '«': 74, '»': 75, 'Y': 76, 'F': 77, 'J': 78, 'ë': 79, 'W': 80, 'X': 81, '0': 82, '5': 83, '7': 84, '9': 85, '6': 86, '3': 87, 'w': 88, 'Î': 89, 'Ç': 90, '4': 91, 'ü': 92, '°': 93}\n",
            "['N', 'o', 'u', 's', ' ', 'é', 't', 'i', 'n', 'à', 'l', \"'\", 'É', 'd', 'e', ',', 'q', 'a', 'P', 'r', 'v', 'h', 'b', 'g', 'ç', 'c', 'p', '.', 'C', 'x', 'm', 'è', '\\n', 'L', 'f', ';', 'î', ':', '-', 'M', 'R', 'j', 'S', 'ù', 'â', 'z', 'I', 'Q', 'û', 'ê', 'O', 'y', 'k', 'ï', 'E', 'T', 'U', 'D', '!', '(', ')', 'B', '?', '_', 'G', '1', '8', '2', 'A', 'À', 'H', 'ô', 'V', 'Ê', '«', '»', 'Y', 'F', 'J', 'ë', 'W', 'X', '0', '5', '7', '9', '6', '3', 'w', 'Î', 'Ç', '4', 'ü', '°']\n",
            "EOP_id = 32\n",
            "Testing mappings consistency...\n",
            "Are 'char_vocabulary' and 'id_to_char' consistent? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc3Gst0zhQv"
      },
      "source": [
        "# Turns a list of lists of ids into a list of strings.\n",
        "\n",
        "def ids_to_texts(ids):\n",
        "  result_texts = []\n",
        "  for seq in ids:\n",
        "    chars = []\n",
        "    for char_id in seq:\n",
        "      if char_id == EOP_id:\n",
        "        break\n",
        "      chars.append(id_to_char[char_id])\n",
        "    result_texts.append(\"\".join(chars))\n",
        "  return result_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAhzb4_3RTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7a988b-141b-4401-b872-c4def0fcb179"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[61, 1, 8, 41, 1, 2, 19, 27], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnT7T-yHEPCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a05359a-7956-451b-dcb4-1af432306999"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "ids[0].extend([EOP_id, len(char_vocabulary), len(char_vocabulary)])\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[61, 1, 8, 41, 1, 2, 19, 27, 32, 94, 94], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH4g-Fkkrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c639ecb0-7e1c-4fee-a438-3d9df2fe60d0"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "class BatchGenerator:\n",
        "\n",
        "  def __init__(self, paragraphs, char_vocabulary):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.char_vocabulary = char_vocabulary # dict[str, int]\n",
        "    self.padding_idx = len(char_vocabulary)\n",
        "\n",
        "  def length(self):\n",
        "    return (len(self.paragraphs) - 1)\n",
        "\n",
        "  def get_batch(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "    paragraph_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some paragraph ids.\n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids)\n",
        "\n",
        "  # paragraph_ids: sequence of int·s\n",
        "  def _ids_to_batch(self, paragraph_ids):\n",
        "    firsts = [] # list[list[int]]; first paragraph of each pair\n",
        "    seconds = [] # list[list[int]], second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      firsts.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id]])\n",
        "      seconds.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id + 1]])\n",
        "\n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long)\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long)\n",
        "\n",
        "    return (firsts, seconds)\n",
        "\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "\n",
        "  def all_batches(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "      i += batch_size\n",
        "\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = []\n",
        "    for paragraph in paragraphs:\n",
        "        tmp = [] # list[int]\n",
        "        for char in paragraph:\n",
        "          # Unknown characters are ignored (removed).\n",
        "          if(char in self.char_vocabulary): tmp.append(self.char_vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "\n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, char_vocabulary=char_vocabulary)\n",
        "print(batch_generator.length())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YSugGI7z3JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4d3da9-9ef6-453e-bac5-752e6a081151"
      },
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3)\n",
        "print(ids_to_texts(firsts))\n",
        "print(ids_to_texts(seconds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Il venait offrir ses services, eu égard à la fatale circonstance. Emma répondit qu'elle croyait pouvoir s'en passer. Le marchand ne se tint pas pour battu.\", \"Et il agita son journal en les regardant s'éloigner.\", \"L'idée de revoir les lieux où s'était passée sa jeunesse l'exaltait sans doute, car tout le long du chemin il n'arrêta pas de discourir; puis, à peine arrivé, il sauta vivement de la voiture pour se mettre en quête de Léon; et le clerc eut beau se débattre, M. Homais l'entraîna vers le grand café de Normandie, où il entra majestueusement sans retirer son chapeau, estimant fort provincial de se découvrir dans un endroit public.\"]\n",
            "['-- Mille excuses, dit-il; je désirerais avoir un entretien particulier.', \"Dès qu'il sentit la terre, le cheval d'Emma prit le galop. Rodolphe galopait à côté d'elle. Par moments ils échangeaient une parole. La figure un peu baissée, la main haute et le bras droit déployé, elle s'abandonnait à la cadence du mouvement qui la berçait sur la selle.\", \"Emma attendit Léon trois quarts d'heure. Enfin elle courut à son étude, et, perdue dans toute sorte de conjectures, l'accusant d'indifférence et se reprochant à elle-même sa faiblesse, elle passa l'après-midi le front collé contre les carreaux.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfRCXQOOm8X"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, size_vocabulary, EOP_id, embedding_dim, lstm_hidden_size, lstm_layers, device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.EOP_id = EOP_id\n",
        "\n",
        "    # (i) Embedding layer:\n",
        "    self.padding_idx = size_vocabulary\n",
        "    self.char_embeddings = torch.nn.Embedding(\n",
        "        num_embeddings=size_vocabulary + 1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        padding_idx=self.padding_idx\n",
        "    ).to(self.device)\n",
        "\n",
        "    # (ii) Bidirectional LSTM encoder:\n",
        "    self.encoder_lstm = torch.nn.LSTM(\n",
        "        input_size=embedding_dim,\n",
        "        hidden_size=lstm_hidden_size,\n",
        "        num_layers=lstm_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "    ).to(self.device)\n",
        "\n",
        "    # (iii) Unidirectional LSTM decoder:\n",
        "    self.decoder_lstm = torch.nn.LSTM(\n",
        "        input_size=embedding_dim,\n",
        "        hidden_size=lstm_hidden_size,\n",
        "        num_layers=lstm_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=False\n",
        "    ).to(self.device)\n",
        "\n",
        "    # (iv) Decoder initialiser:\n",
        "    self.decoder_initialiser = torch.nn.Linear(\n",
        "        in_features=lstm_hidden_size * 2,\n",
        "        out_features=lstm_hidden_size\n",
        "    ).to(self.device)\n",
        "\n",
        "    # (v) Distribution layer:\n",
        "    self.distribution_nn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(lstm_hidden_size, size_vocabulary),\n",
        "        torch.nn.LogSoftmax(dim=-1)\n",
        "    ).to(self.device)\n",
        "\n",
        "  def initStates(self, in_paragraphs):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.char_embeddings(in_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_h_n); print(bi_h_n.shape)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(c_n); print(c_n.shape)\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_c_n); print(bi_c_n.shape)\n",
        "\n",
        "    return (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n))\n",
        "\n",
        "  def trainingLogits(self, in_paragraphs, out_paragraphs):\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    h_initial, c_initial = decoder_init_states  # Shapes: (num_layers, batch_size, hidden_size) each\n",
        "\n",
        "    initial_logits = self.distribution_nn(h_initial[-1])  # (batch_size, vocab_size)\n",
        "    initial_logits = initial_logits.unsqueeze(1)  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    # Prepare decoder inputs: out_paragraphs[:, :-1] are the tokens before the last one (teacher forcing).\n",
        "    decoder_input = out_paragraphs[:, :-1]  # shape: (batch_size, max_out_len-1)\n",
        "    out_embeddings = self.char_embeddings(decoder_input)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    # Compute lengths for packed sequence (excluding padding_idx).\n",
        "    out_lengths = (decoder_input != self.char_embeddings.padding_idx).sum(axis=1)  # (batch_size)\n",
        "    out_embeddings_packed = torch.nn.utils.rnn.pack_padded_sequence(out_embeddings, lengths=out_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    # Run the decoder LSTM\n",
        "    decoder_out_packed, _ = self.decoder_lstm(out_embeddings_packed, (h_initial, c_initial))\n",
        "\n",
        "    # Unpack the sequences\n",
        "    decoder_out, _ = torch.nn.utils.rnn.pad_packed_sequence(decoder_out_packed, batch_first=True)\n",
        "    # decoder_out shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # Compute distributions for each time step in decoder_out\n",
        "    decoder_logits = self.distribution_nn(decoder_out)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "    # Concatenate initial_logits with decoder_logits\n",
        "    logits = torch.cat([initial_logits, decoder_logits], dim=1)  # (batch_size, seq_len+1, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def predictionStrings(self, in_paragraphs, max_predicted_char=1000):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    decoder_init_states = self.initStates(in_paragraphs)\n",
        "\n",
        "    h_initial, c_initial = decoder_init_states  # (num_layers, batch_size, hidden_size) each\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    # Storage for generated characters and log-probabilities\n",
        "    char_ids = []\n",
        "    logprobs = torch.zeros(batch_size, device=self.device)\n",
        "\n",
        "    # Compute initial logits\n",
        "    initial_logits = self.distribution_nn(h_initial[-1])  # (batch_size, vocab_size)\n",
        "    # Greedy selection of the first character\n",
        "    first_char = torch.argmax(initial_logits, dim=1)  # (batch_size)\n",
        "    char_ids.append(first_char)\n",
        "\n",
        "    # Gather log-prob for the chosen character\n",
        "    first_char_logprob = initial_logits.gather(1, first_char.unsqueeze(1)).squeeze(1)  # (batch_size)\n",
        "    logprobs += first_char_logprob\n",
        "\n",
        "    # Current decoder states\n",
        "    h_current, c_current = h_initial, c_initial\n",
        "\n",
        "    # Current input character for the decoder is the chosen first_char\n",
        "    current_char = first_char.unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "    # Generate subsequent characters\n",
        "    for _ in range(max_predicted_char - 1):\n",
        "      # Embed the current character\n",
        "      current_char_emb = self.char_embeddings(current_char)  # (batch_size, 1, embedding_dim)\n",
        "\n",
        "      # Pass it through the decoder\n",
        "      decoder_out, (h_current, c_current) = self.decoder_lstm(current_char_emb, (h_current, c_current))\n",
        "      # decoder_out: (batch_size, 1, hidden_size)\n",
        "\n",
        "      # Compute distribution\n",
        "      step_logits = self.distribution_nn(decoder_out.squeeze(1))  # (batch_size, vocab_size)\n",
        "      # Greedy selection of next character\n",
        "      next_char = torch.argmax(step_logits, dim=1)  # (batch_size)\n",
        "\n",
        "      # Append next_char to char_ids\n",
        "      char_ids.append(next_char)\n",
        "\n",
        "      # Gather log-prob for the chosen character\n",
        "      step_logprob = step_logits.gather(1, next_char.unsqueeze(1)).squeeze(1)  # (batch_size)\n",
        "      logprobs += step_logprob\n",
        "\n",
        "      # Update current_char\n",
        "      current_char = next_char.unsqueeze(1)\n",
        "\n",
        "      # Stop if all sequences ended\n",
        "      if (next_char == self.EOP_id).all():\n",
        "        break\n",
        "\n",
        "    # Convert char_ids (list of length T with tensors of shape (batch_size)) to a (batch_size, T) tensor\n",
        "    char_ids = torch.stack(char_ids, dim=1)  # (batch_size, seq_length)\n",
        "\n",
        "    # Convert IDs to texts\n",
        "    texts = ids_to_texts(char_ids.cpu().numpy())  # List[str] of length batch_size\n",
        "\n",
        "    return texts, logprobs  # List[str], Tensor[float]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TXsvizgogZ"
      },
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=19, lstm_hidden_size=13, lstm_layers=7, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sp6c9Bdch3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe529a9f-4ca5-4a7f-a964-8e22fb1988ce"
      },
      "source": [
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device) # A batch that contains only one sentence with no padding.\n",
        "print(in_paragraphs)\n",
        "\n",
        "out_paragraphs = in_paragraphs\n",
        "logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "print(f\"logits:\\n{logits}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]], device='cuda:0')\n",
            "logits:\n",
            "tensor([[[-4.4215, -4.4793, -4.4751, -4.6891, -4.7921, -4.6214, -4.4352,\n",
            "          -4.2475, -4.7300, -4.7387, -4.7140, -4.5036, -4.6740, -4.4603,\n",
            "          -4.3544, -4.4323, -4.5620, -4.3372, -4.3993, -4.4632, -4.2614,\n",
            "          -4.7583, -4.8507, -4.4887, -4.4829, -4.8096, -4.4013, -4.3509,\n",
            "          -4.5843, -4.3103, -4.5214, -4.6296, -4.4320, -4.6118, -4.4104,\n",
            "          -4.4667, -4.7839, -4.6559, -4.3442, -4.3667, -4.7344, -4.8644,\n",
            "          -4.3967, -4.2981, -4.4585, -4.3590, -4.6204, -4.7793, -4.4302,\n",
            "          -4.2422, -4.3399, -4.6321, -4.3782, -4.7910, -4.8586, -4.5245,\n",
            "          -4.5083, -4.3943, -4.8245, -4.5321, -4.3653, -4.7984, -4.8495,\n",
            "          -4.7530, -4.5563, -4.2409, -4.8063, -4.8116, -4.3400, -4.7272,\n",
            "          -4.5926, -4.3218, -4.4756, -4.5114, -4.5680, -4.7000, -4.7760,\n",
            "          -4.2920, -4.3251, -4.5537, -4.8072, -4.5320, -4.3566, -4.8008,\n",
            "          -4.7067, -4.5939, -4.5739, -4.8739, -4.4575, -4.7493, -4.6235,\n",
            "          -4.7512, -4.6263, -4.7477],\n",
            "         [-4.3166, -4.4929, -4.5326, -4.6636, -4.8036, -4.6148, -4.4356,\n",
            "          -4.3107, -4.7521, -4.7477, -4.6365, -4.5504, -4.7242, -4.4495,\n",
            "          -4.4310, -4.3818, -4.5502, -4.3431, -4.4247, -4.4796, -4.3601,\n",
            "          -4.6930, -4.8158, -4.6079, -4.5081, -4.7269, -4.4765, -4.3312,\n",
            "          -4.5383, -4.2533, -4.5577, -4.7306, -4.5538, -4.5833, -4.4355,\n",
            "          -4.4789, -4.8182, -4.6648, -4.4162, -4.3958, -4.7106, -4.7918,\n",
            "          -4.3750, -4.3188, -4.3465, -4.3460, -4.6195, -4.7416, -4.4810,\n",
            "          -4.2571, -4.4101, -4.6119, -4.2842, -4.6817, -4.7763, -4.5036,\n",
            "          -4.4129, -4.3780, -4.6568, -4.5535, -4.4118, -4.7504, -4.8293,\n",
            "          -4.7034, -4.6207, -4.2454, -4.7281, -4.7976, -4.3064, -4.6039,\n",
            "          -4.6071, -4.3398, -4.4354, -4.5202, -4.6608, -4.7022, -4.8107,\n",
            "          -4.2898, -4.3953, -4.5648, -4.7614, -4.6565, -4.3852, -4.7946,\n",
            "          -4.6972, -4.6373, -4.6720, -4.7873, -4.4065, -4.7640, -4.5844,\n",
            "          -4.6538, -4.7010, -4.6892],\n",
            "         [-4.2882, -4.5022, -4.5525, -4.6620, -4.8361, -4.5870, -4.4655,\n",
            "          -4.3165, -4.7295, -4.7416, -4.5934, -4.5598, -4.7468, -4.4265,\n",
            "          -4.4696, -4.3922, -4.5285, -4.3305, -4.4217, -4.4864, -4.3829,\n",
            "          -4.6849, -4.8061, -4.6513, -4.5186, -4.7176, -4.4930, -4.3519,\n",
            "          -4.5026, -4.2312, -4.5766, -4.7521, -4.6071, -4.5918, -4.4215,\n",
            "          -4.4753, -4.8447, -4.6435, -4.4322, -4.4153, -4.7029, -4.7694,\n",
            "          -4.3734, -4.3278, -4.3054, -4.3369, -4.6222, -4.7204, -4.5284,\n",
            "          -4.2331, -4.4513, -4.6160, -4.2387, -4.6377, -4.7413, -4.5202,\n",
            "          -4.3709, -4.3824, -4.6133, -4.5829, -4.4525, -4.7344, -4.8497,\n",
            "          -4.7010, -4.6546, -4.2272, -4.7133, -4.7939, -4.2921, -4.5751,\n",
            "          -4.5880, -4.3587, -4.4267, -4.5088, -4.6666, -4.6963, -4.8419,\n",
            "          -4.2722, -4.4315, -4.5768, -4.7461, -4.7049, -4.3712, -4.7807,\n",
            "          -4.7208, -4.6601, -4.6799, -4.7544, -4.4079, -4.7754, -4.5746,\n",
            "          -4.6491, -4.7248, -4.6697],\n",
            "         [-4.2696, -4.5130, -4.5622, -4.6570, -4.8578, -4.5681, -4.4830,\n",
            "          -4.3157, -4.7076, -4.7385, -4.5700, -4.5692, -4.7619, -4.4073,\n",
            "          -4.4938, -4.4003, -4.5131, -4.3190, -4.4190, -4.4918, -4.4022,\n",
            "          -4.6740, -4.7962, -4.6814, -4.5197, -4.7052, -4.5062, -4.3636,\n",
            "          -4.4754, -4.2166, -4.5903, -4.7636, -4.6370, -4.5975, -4.4133,\n",
            "          -4.4717, -4.8630, -4.6259, -4.4442, -4.4282, -4.6978, -4.7602,\n",
            "          -4.3738, -4.3361, -4.2812, -4.3324, -4.6286, -4.7020, -4.5580,\n",
            "          -4.2201, -4.4748, -4.6137, -4.2105, -4.6141, -4.7153, -4.5329,\n",
            "          -4.3511, -4.3935, -4.5853, -4.6044, -4.4789, -4.7236, -4.8632,\n",
            "          -4.6974, -4.6766, -4.2190, -4.7024, -4.7890, -4.2815, -4.5651,\n",
            "          -4.5726, -4.3719, -4.4235, -4.5019, -4.6711, -4.6937, -4.8628,\n",
            "          -4.2606, -4.4500, -4.5858, -4.7388, -4.7353, -4.3573, -4.7742,\n",
            "          -4.7394, -4.6787, -4.6858, -4.7363, -4.4148, -4.7877, -4.5690,\n",
            "          -4.6493, -4.7454, -4.6573],\n",
            "         [-4.2593, -4.5220, -4.5657, -4.6524, -4.8720, -4.5555, -4.4926,\n",
            "          -4.3143, -4.6913, -4.7357, -4.5578, -4.5773, -4.7714, -4.3942,\n",
            "          -4.5082, -4.4067, -4.5036, -4.3103, -4.4164, -4.4953, -4.4158,\n",
            "          -4.6645, -4.7880, -4.7018, -4.5183, -4.6952, -4.5141, -4.3709,\n",
            "          -4.4571, -4.2069, -4.5991, -4.7696, -4.6535, -4.6008, -4.4089,\n",
            "          -4.4680, -4.8739, -4.6120, -4.4515, -4.4365, -4.6945, -4.7578,\n",
            "          -4.3755, -4.3430, -4.2671, -4.3297, -4.6333, -4.6887, -4.5746,\n",
            "          -4.2136, -4.4867, -4.6105, -4.1942, -4.6006, -4.6981, -4.5402,\n",
            "          -4.3433, -4.4039, -4.5677, -4.6197, -4.4946, -4.7164, -4.8720,\n",
            "          -4.6950, -4.6906, -4.2155, -4.6962, -4.7848, -4.2733, -4.5633,\n",
            "          -4.5617, -4.3802, -4.4219, -4.4985, -4.6735, -4.6935, -4.8767,\n",
            "          -4.2541, -4.4590, -4.5916, -4.7355, -4.7531, -4.3461, -4.7715,\n",
            "          -4.7528, -4.6917, -4.6894, -4.7260, -4.4223, -4.7984, -4.5653,\n",
            "          -4.6510, -4.7610, -4.6499]]], device='cuda:0',\n",
            "       grad_fn=<CatBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks that a tensor is a tensor of logits, i.e. that each vector is a vector of log-probabilities.\n",
        "def check_logits(logits):\n",
        "\n",
        "  # Convert from log probabilities to probabilities\n",
        "  probs = torch.exp(logits)\n",
        "  # Sum probabilities along the last dimension\n",
        "  row_sums = probs.sum(dim=-1)\n",
        "  # Check if all sums are close to 1 within a numerical tolerance\n",
        "  return torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5, rtol=1e-5)\n",
        "\n",
        "print(f\"The following should be True: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.2], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.0], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.6, 2.0], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.2], [0.1, 0.2, 0.9]])))}\")"
      ],
      "metadata": {
        "id": "ww6UwsI2cuPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a8c51e-a0d1-4f29-819c-70590ccbef5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following should be True: True\n",
            "The following should be False: False\n",
            "The following should be False: False\n",
            "The following should be False: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The following should be True: {check_logits(logits)}\")"
      ],
      "metadata": {
        "id": "6bVNvxt5czmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49454570-ed9a-4f90-ce24-3fb0942ca878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device) # A batch that contains two sentences with some padding (more than necessary).\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "print(logits)\n",
        "\n",
        "print(f\"The following should be True: {check_logits(logits)}\")"
      ],
      "metadata": {
        "id": "ftAttD70si-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232dd6b7-c170-46f3-ea52-dfc2cda3354b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94],\n",
            "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 94, 94, 94, 94, 94]],\n",
            "       device='cuda:0')\n",
            "tensor([[[-4.4215, -4.4793, -4.4751,  ..., -4.7512, -4.6263, -4.7477],\n",
            "         [-4.3166, -4.4929, -4.5326,  ..., -4.6538, -4.7010, -4.6892],\n",
            "         [-4.2882, -4.5022, -4.5525,  ..., -4.6491, -4.7248, -4.6697],\n",
            "         ...,\n",
            "         [-4.3327, -4.4988, -4.4922,  ..., -4.6600, -4.7188, -4.7048],\n",
            "         [-4.3327, -4.4988, -4.4922,  ..., -4.6600, -4.7188, -4.7048],\n",
            "         [-4.3327, -4.4988, -4.4922,  ..., -4.6600, -4.7188, -4.7048]],\n",
            "\n",
            "        [[-4.4151, -4.4808, -4.4845,  ..., -4.7503, -4.6277, -4.7437],\n",
            "         [-4.3127, -4.4936, -4.5378,  ..., -4.6536, -4.7022, -4.6873],\n",
            "         [-4.2860, -4.5025, -4.5555,  ..., -4.6492, -4.7256, -4.6690],\n",
            "         ...,\n",
            "         [-4.2500, -4.5376, -4.5634,  ..., -4.6550, -4.7873, -4.6395],\n",
            "         [-4.2500, -4.5387, -4.5624,  ..., -4.6552, -4.7891, -4.6385],\n",
            "         [-4.2502, -4.5394, -4.5617,  ..., -4.6552, -4.7903, -4.6379]]],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "The following should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPGm_Duq_QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d89571-a537-4f5c-ae1c-8ef45172550a"
      },
      "source": [
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2)\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ê11kkkkkkkkkkkkk', 'ê11kkkkkkkkkkkkk'],\n",
              " tensor([-67.0464, -67.0462], device='cuda:0', grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJSBtNGCX-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b92e120-b18b-45dc-c086-bedbe002e7fd"
      },
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=256, lstm_hidden_size=512, lstm_layers=1, device='cuda')\n",
        "\n",
        "import time\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.2\n",
        "momentum = 0.99\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "batch_size = 64 if(not use_toy_dataset) else 128\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length() if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 50 if(not use_toy_dataset) else 3\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "average_losses = []  # List to store average loss for each epoch\n",
        "time_0 = time.time()\n",
        "\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train()\n",
        "\n",
        "  model.zero_grad()\n",
        "  batch = batch_generator.get_batch(batch_size, subset=subset)\n",
        "  #print(ids_to_texts(batch[0])); print(ids_to_texts(batch[1]))\n",
        "  in_paragraphs = batch[0].to(model.device)\n",
        "  out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "  # (i) Compute predictions:\n",
        "  logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "\n",
        "  # (ii) Compute the loss:\n",
        "  batch_size, seq_len, vocab_size = logits.shape\n",
        "  logits_2d = logits.view(-1, vocab_size)              # (batch_size*seq_len, vocab_size)\n",
        "  targets = out_paragraphs.view(-1)                    # (batch_size*seq_len)\n",
        "\n",
        "  # Ignore padding index to not compute loss on padded characters.\n",
        "  loss = torch.nn.functional.nll_loss(\n",
        "      logits_2d,\n",
        "      targets,\n",
        "      ignore_index=batch_generator.padding_idx,\n",
        "      reduction='mean'\n",
        "  )\n",
        "\n",
        "  # (iii) Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # (iv) Store the loss\n",
        "  epoch_loss.append(loss.item())\n",
        "\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size):\n",
        "    # Calculate and store the average loss for the epoch\n",
        "    avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
        "    average_losses.append(avg_loss)\n",
        "\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {avg_loss}.\")\n",
        "    duration = time.time() - time_0\n",
        "    print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "\n",
        "    # Example of generation\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      batch = batch_generator.get_batch(1, subset=subset)\n",
        "      print(ids_to_texts(batch[0])) # Input paragraph\n",
        "      print(model.predictionStrings(batch[0].to(model.device), max_predicted_char=512)) # Generated output paragraph.\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- END OF EPOCH 0.\n",
            "Average loss: 2.913053715482671.\n",
            "21.536670207977295 s elapsed (i.e. 21.536670207977295 s/epoch)\n",
            "[\"-- Embrassez-moi, dit l'apothicaire les larmes aux yeux. Voilà votre paletot, mon bon ami; prenez garde au froid! Soignez-vous! ménagez-vous!\"]\n",
            "(['---- le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le '], tensor([-544.2258], device='cuda:0'))\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.189587522060313.\n",
            "44.270968198776245 s elapsed (i.e. 22.135484099388123 s/epoch)\n",
            "[\"Il se tut, cherchant des yeux un public autour de lui, car, dans son effervescence, le pharmacien un moment s'était cru en plein conseil municipal. Mais la maîtresse d'auberge ne l'écoutait plus; elle tendait son oreille à un roulement éloigné. On distingua le bruit d'une voiture mêlé à un claquement de fers lâches qui battaient la terre, et l'Hirondelle enfin s'arrêta devant la porte.\"]\n",
            "(['Il sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa'], tensor([-625.6765], device='cuda:0'))\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 1.970251496801985.\n",
            "66.63472843170166 s elapsed (i.e. 22.211576143900555 s/epoch)\n",
            "[\"Comme il avait eu longtemps l'habitude du bonnet de coton, son foulard ne lui tenait pas aux oreilles; aussi ses cheveux, le matin, étaient rabattus pêle-mêle sur sa figure et blanchis par le duvet de son oreiller, dont les cordons se dénouaient pendant la nuit. Il portait toujours de fortes bottes, qui avaient au cou-de-pied deux plis épais obliquant vers les chevilles, tandis que le reste de l'empeigne se continuait en ligne droite, tendu comme par un pied de bois. Il disait que c'était bien assez bon pour la campagne.\"]\n",
            "(['Elle pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas'], tensor([-567.3209], device='cuda:0'))\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 1.8383094630342849.\n",
            "91.08192491531372 s elapsed (i.e. 22.77048122882843 s/epoch)\n",
            "['Elle eut un sanglot.']\n",
            "([\"-- C'est par les se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se se \"], tensor([-725.8027], device='cuda:0'))\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 1.7483001221781191.\n",
            "113.48426556587219 s elapsed (i.e. 22.696853113174438 s/epoch)\n",
            "[\"Il s'ennuyait maintenant lorsque Emma, tout à coup, sanglotait sur sa poitrine; et son coeur, comme les gens qui ne peuvent endurer qu'une certaine dose de musique, s'assoupissait d'indifférence au vacarme d'un amour dont il ne distinguait plus les délicatesses.\"]\n",
            "(['-- Ah! comme de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la prosses de la pro'], tensor([-563.7401], device='cuda:0'))\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.68309940429444.\n",
            "138.16981172561646 s elapsed (i.e. 23.02830195426941 s/epoch)\n",
            "['-- Comment, monsieur Boulanger, vous nous abandonnez?']\n",
            "(['-- Ah! comme de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la'], tensor([-452.7011], device='cuda:0'))\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.627024161054733.\n",
            "160.63932514190674 s elapsed (i.e. 22.94847502027239 s/epoch)\n",
            "[\"Jusqu'en 1835, il n'y avait point de route praticable pour arriver à Yonville; mais on a établi vers cette époque un chemin de grande vicinalité qui relie la route d'Abbeville à celle d'Amiens, et sert quelquefois aux rouliers allant de Rouen dans les Flandres. Cependant, Yonville-l'Abbaye est demeuré stationnaire, malgré ses débouchés nouveaux. Au lieu d'améliorer les cultures, on s'y obstine encore aux herbages, quelque dépréciés qu'ils soient, et le bourg paresseux, s'écartant de la plaine, a continué naturellement à s'agrandir vers la rivière. On l'aperçoit de loin, tout couché en long sur la rive, comme un gardeur de vaches qui fait la sieste au bord de l'eau.\"]\n",
            "(['-- Ah! courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de la pendant les courait de '], tensor([-450.8964], device='cuda:0'))\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.5802537152107725.\n",
            "185.7507266998291 s elapsed (i.e. 23.218840837478638 s/epoch)\n",
            "[\"Quand Justin, qui se rhabillait, fut parti, l'on causa quelque peu des évanouissements. Madame Bovary n'en avait jamais eu.\"]\n",
            "(['Le charmacien de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de la chanter les proches de l'], tensor([-501.3879], device='cuda:0'))\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.5497446821091023.\n",
            "208.94166111946106 s elapsed (i.e. 23.215740124384563 s/epoch)\n",
            "[\"-- Pas encore; mais je les verrai l'année prochaine, quand j'irai habiter Paris, pour finir mon droit.\"]\n",
            "([\"-- Ah! n'avait de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient de la campagnes de la couraient d\"], tensor([-458.0720], device='cuda:0'))\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.5105103928109873.\n",
            "231.73776721954346 s elapsed (i.e. 23.173776721954347 s/epoch)\n",
            "[\"Mais elle reprit vivement, à voix basse, d'une voix douce, dissolvante:\"]\n",
            "([\"-- Ah! n'est pas de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche de la proche d\"], tensor([-456.4763], device='cuda:0'))\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.4731934653951766.\n",
            "255.67375707626343 s elapsed (i.e. 23.243068825114857 s/epoch)\n",
            "[\"Elle but en arrivant un grand verre d'eau. Elle était très pâle. Elle lui dit:\"]\n",
            "([\"-- Ah! n'est pas de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les conseilles de la chez le contre les co\"], tensor([-460.8970], device='cuda:0'))\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.4434403612258586.\n",
            "279.73099637031555 s elapsed (i.e. 23.310916364192963 s/epoch)\n",
            "[\"Comme elle se plaignait de Tostes continuellement, Charles imagina que la cause de sa maladie était sans doute dans quelque influence locale, et, s'arrêtant à cette idée, il songea sérieusement à aller s'établir ailleurs.\"]\n",
            "(['-- Celle se retournait de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considérations de la courait des considéra'], tensor([-378.0752], device='cuda:0'))\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.4087465164509225.\n",
            "304.17166662216187 s elapsed (i.e. 23.397820509397068 s/epoch)\n",
            "['-- Serviteur! dit-il, je suis à vous.']\n",
            "(['-- Ah! nous de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la co'], tensor([-483.8930], device='cuda:0'))\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.378453202869581.\n",
            "327.4233031272888 s elapsed (i.e. 23.387378794806345 s/epoch)\n",
            "['-- Ma cas... fit timidement le nouveau, promenant autour de lui des regards inquiets.']\n",
            "([\"-- Ce n'est pas de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la courant de la \"], tensor([-440.4586], device='cuda:0'))\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.3527585572384773.\n",
            "350.70098662376404 s elapsed (i.e. 23.380065774917604 s/epoch)\n",
            "['Et il reprit']\n",
            "(['-- Ah! comme des considérations de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son contre les partiers de la conseille de son '], tensor([-417.4211], device='cuda:0'))\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.325690396288608.\n",
            "374.50718212127686 s elapsed (i.e. 23.406698882579803 s/epoch)\n",
            "['Elle frissonna.']\n",
            "([\"-- C'est pas la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la chambre, et le considérait de la c\"], tensor([-386.1829], device='cuda:0'))\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.304570712941758.\n",
            "397.7032370567322 s elapsed (i.e. 23.394308062160718 s/epoch)\n",
            "['-- Enfant que vous êtes! Allons, soyons sage je le veux!']\n",
            "([\"-- Ce n'est pas de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait de la chambre, elle avait pas de la compagne, et le promenait \"], tensor([-352.6631], device='cuda:0'))\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.2888535844518783.\n",
            "421.7055068016052 s elapsed (i.e. 23.42808371120029 s/epoch)\n",
            "[\"-- On prétend, qu'ils sentent les morts, répondit l'ecclésiastique. C'est comme les abeilles: elles s'envolent de la ruche au décès des personnes. Homais ne releva pas ces préjugés, car il s'était rendormi.\"]\n",
            "([\"-- Ah! ce n'est pas de la considération de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses consides de ses c\"], tensor([-416.3588], device='cuda:0'))\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.26413006886192.\n",
            "447.01920318603516 s elapsed (i.e. 23.527326483475534 s/epoch)\n",
            "[\"Madame Bovary mère ne trouvait rien à blâmer, sauf peut-être cette manie de tricoter des camisoles pour les orphelins, au lieu de raccommoder ses torchons. Mais, harassée de querelles domestiques, la bonne femme se plaisait en cette maison tranquille, et même elle y demeura jusques après Pâques, afin d'éviter les sarcasmes du père Bovary, qui ne manquait pas, tous les vendredis saints, de se commander une andouille.\"]\n",
            "([\"-- C'est une femme en parlant de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son coup, et le considérable de son chapeau de son coup de son cou\"], tensor([-401.0114], device='cuda:0'))\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.251114157920188.\n",
            "471.3623526096344 s elapsed (i.e. 23.56811763048172 s/epoch)\n",
            "['-- Vous êtes tous des infâmes!']\n",
            "(['-- Comment! vous avoir dire le calme de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la chaise de la c'], tensor([-381.3376], device='cuda:0'))\n",
            "-- END OF EPOCH 20.\n",
            "Average loss: 1.2348264582613682.\n",
            "494.94557905197144 s elapsed (i.e. 23.568837097712926 s/epoch)\n",
            "[\"-- Pourtant elle s'occupe, disait Charles.\"]\n",
            "([\"-- Et il le revenait de l'autre de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le prier de la chambre, et le \"], tensor([-388.1947], device='cuda:0'))\n",
            "-- END OF EPOCH 21.\n",
            "Average loss: 1.2204565951164732.\n",
            "518.7131879329681 s elapsed (i.e. 23.57787217877128 s/epoch)\n",
            "[\"-- C'est encore moi! dit Léon.\"]\n",
            "(['-- Eh! comme le parler de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la prochaine de la pro'], tensor([-397.6701], device='cuda:0'))\n",
            "-- END OF EPOCH 22.\n",
            "Average loss: 1.209442594776983.\n",
            "540.9941213130951 s elapsed (i.e. 23.521483535351962 s/epoch)\n",
            "[\"L'odeur du gaz se mêlait aux haleines; le vent des éventails rendait l'atmosphère plus étouffante. Emma voulut sortir; la foule encombrait les corridors, et elle retomba dans son fauteuil avec des palpitations qui la suffoquaient. Charles, ayant peur de la voir s'évanouir, courut à la buvette lui chercher un verre d'orgeat.\"]\n",
            "([\"-- Ah! c'est une mariage en souvenir de la première courte, et le précipitait de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de son coup de\"], tensor([-402.5988], device='cuda:0'))\n",
            "-- END OF EPOCH 23.\n",
            "Average loss: 1.1990227268097249.\n",
            "564.665105342865 s elapsed (i.e. 23.527712722619373 s/epoch)\n",
            "[\"-- Hélas! ma pauvre chère dame, c'est qu'il a, par suite de ses blessures, des crampes terribles à la poitrine. Il dit même que le cidre l'affaiblit.\"]\n",
            "([\"-- Ce n'est pas comme une fois qu'elle ne pas pas comme une fois qu'elle avait pas de l'autre comme des chambres de la place de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de son pied de \"], tensor([-417.1699], device='cuda:0'))\n",
            "-- END OF EPOCH 24.\n",
            "Average loss: 1.1823987174541393.\n",
            "589.1262674331665 s elapsed (i.e. 23.56505069732666 s/epoch)\n",
            "[\"-- Si j'en cueillais. Qu'en pensez-vous?\"]\n",
            "(['-- Comme est moi!'], tensor([-13.3889], device='cuda:0'))\n",
            "-- END OF EPOCH 25.\n",
            "Average loss: 1.1664177103245512.\n",
            "611.9382219314575 s elapsed (i.e. 23.536085458902214 s/epoch)\n",
            "[\"Le nouveau articula, d'une voix bredouillante, un nom inintelligible.\"]\n",
            "([\"-- Ah! c'est un peu de l'autre, comme si le coeur de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le cour de la chambre, et le\"], tensor([-381.7270], device='cuda:0'))\n",
            "-- END OF EPOCH 26.\n",
            "Average loss: 1.1606173134864646.\n",
            "636.0656545162201 s elapsed (i.e. 23.55798720430445 s/epoch)\n",
            "['Félicité sanglotait:']\n",
            "([\"-- Ah! c'est un peu de la campagne, de ses candeaux de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les parties de la campagne, et les par\"], tensor([-364.5757], device='cuda:0'))\n",
            "-- END OF EPOCH 27.\n",
            "Average loss: 1.1483325595441072.\n",
            "658.49951338768 s elapsed (i.e. 23.517839763845718 s/epoch)\n",
            "[\"Rodolphe réfléchit beaucoup à cette histoire de pistolets. Si elle avait parlé sérieusement, cela était fort ridicule, pensait-il, odieux même, car il n'avait, lui, aucune raison de haïr ce bon Charles, n'étant pas ce qui s'appelle dévoré de jalousie; -- et, à ce propos, Emma lui avait fait un grand serment qu'il ne trouvait pas non plus du meilleur goût.\"]\n",
            "([\"-- Ah! c'est un mari de la poitrine de la porte de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers de la chambre, et les premiers\"], tensor([-335.8620], device='cuda:0'))\n",
            "-- END OF EPOCH 28.\n",
            "Average loss: 1.1325543535516618.\n",
            "680.1848392486572 s elapsed (i.e. 23.454649629264043 s/epoch)\n",
            "[\"Charles finissait par s'estimer davantage de ce qu'il possédait une pareille femme. Il montrait avec orgueil, dans la salle, deux petits croquis d'elle, à la mine de plomb, qu'il avait fait encadrer de cadres très larges et suspendus contre le papier de la muraille à de longs cordons verts. Au sortir de la messe, on le voyait sur sa porte avec de belles pantoufles en tapisserie.\"]\n",
            "(['-- Comment vous ne sais pas trop terre, dit le pharmacien, en effet, comme les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convitures de la chambre, et les mains de ses convit'], tensor([-367.3991], device='cuda:0'))\n",
            "-- END OF EPOCH 29.\n",
            "Average loss: 1.130485970923241.\n",
            "703.6242463588715 s elapsed (i.e. 23.454141545295716 s/epoch)\n",
            "[\"-- Enfin y est-elle? s'écria Tuvache.\"]\n",
            "(['Et il les regardait des cours de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin de la chambre, au coin'], tensor([-340.8559], device='cuda:0'))\n",
            "-- END OF EPOCH 30.\n",
            "Average loss: 1.1170734851918322.\n",
            "727.3650214672089 s elapsed (i.e. 23.463387789264804 s/epoch)\n",
            "[\"On est ici sur les confins de la Normandie, de la Picardie et de l'Île-de-France, contrée bâtarde où le langage est sans accentuation, comme le paysage sans caractère. C'est là que l'on fait les pires fromages de Neufchâtel de tout l'arrondissement, et, d'autre part, la culture y est coûteuse, parce qu'il faut beaucoup de fumier pour engraisser ces terres friables pleines de sable et de cailloux.\"]\n",
            "([\"-- Ah! c'est le présent de la cheminée.\"], tensor([-25.6665], device='cuda:0'))\n",
            "-- END OF EPOCH 31.\n",
            "Average loss: 1.1112475243020565.\n",
            "751.1801497936249 s elapsed (i.e. 23.474379681050777 s/epoch)\n",
            "[\"Le spectacle des objets connus qui défilaient devant ses yeux peu à peu détournait Emma de sa douleur présente. Une intolérable fatigue l'accablait, et elle arriva chez elle hébétée, découragée, presque endormie.\"]\n",
            "([\"-- Oh! c'est vrai! dit-elle.\"], tensor([-15.9081], device='cuda:0'))\n",
            "-- END OF EPOCH 32.\n",
            "Average loss: 1.1036390947258992.\n",
            "773.017984867096 s elapsed (i.e. 23.42478742021503 s/epoch)\n",
            "[\"-- Certainement, je m'y entends, puisque je suis pharmacien, c'est-à-dire chimiste! et la chimie, madame Lefrançois, ayant pour objet la connaissance de l'action réciproque et moléculaire de tous les corps de la nature, il s'ensuit que l'agriculture se trouve comprise dans son domaine! Et, en effet, composition des engrais, fermentation des liquides, analyse des gaz et influence des miasmes, qu'est-ce que tout cela, je vous le demande, si ce n'est de la chimie pure et simple?\"]\n",
            "([\"-- Ah! c'est un peu de ces choses de la cheminée de la conduite et les pantoufles de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour d\"], tensor([-439.6570], device='cuda:0'))\n",
            "-- END OF EPOCH 33.\n",
            "Average loss: 1.0992565408666084.\n",
            "794.5032243728638 s elapsed (i.e. 23.367741893319522 s/epoch)\n",
            "['Et elle restait à faire rougir les pincettes, ou regardant la pluie tomber.']\n",
            "([\"-- Ah! non, non, nous ne peut-êt-il? pas trop fort, et il se regardait à sa main dans la course, et qui s'en retournait à la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sans doute entre les pareils de la porte, sa\"], tensor([-338.3645], device='cuda:0'))\n",
            "-- END OF EPOCH 34.\n",
            "Average loss: 1.0842646740852517.\n",
            "817.3154983520508 s elapsed (i.e. 23.351871381487165 s/epoch)\n",
            "[\"-- Oui, fit-elle avec indifférence; c'est un bouquet que j'ai acheté tantôt... à une mendiante.\"]\n",
            "(['Elle se retirait à ses deux bras de couleur de ces matins de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de couleur de ces paupières de cou'], tensor([-357.8044], device='cuda:0'))\n",
            "-- END OF EPOCH 35.\n",
            "Average loss: 1.0824496669972197.\n",
            "841.523295879364 s elapsed (i.e. 23.375647107760113 s/epoch)\n",
            "[\"N'importe s'il n'avait point demandé d'explications, d'autres plus tard pourraient se montrer moins discrets. Aussi jugea-t-elle utile de descendre chaque fois à la Croix rouge, de sorte que les bonnes gens de son village qui la voyaient dans l'escalier ne se doutaient de rien.\"]\n",
            "(['-- Comment! dit-elle.'], tensor([-10.3249], device='cuda:0'))\n",
            "-- END OF EPOCH 36.\n",
            "Average loss: 1.0787379741668701.\n",
            "863.702778339386 s elapsed (i.e. 23.343318333496917 s/epoch)\n",
            "[\"-- Quel entêtement tu as quelquefois! J'ai été à Barfeuchères aujourd'hui. Eh bien, madame Liégeard m'a certifié que ses trois demoiselles, qui sont à la Miséricorde, prenaient des leçons moyennant cinquante sous la séance, et d'une fameuse maîtresse encore!\"]\n",
            "([\"-- Ah! c'est une manière de sa maison.\"], tensor([-26.3450], device='cuda:0'))\n",
            "-- END OF EPOCH 37.\n",
            "Average loss: 1.0666318294849801.\n",
            "888.5068204402924 s elapsed (i.e. 23.381758432639273 s/epoch)\n",
            "['Il chantait une petite chanson en suivant les voitures:']\n",
            "([\"-- Oui, je vous ai bien vous remontre le matin d'où est si bien que le matin d'avance s'avançant de l'été de la conversation s'endormit dans le soir, en souvenir de ses boutiques de sa femme, et s'en retourna dans le soir, en souvenir de ses boutiques de sa femme, et s'en retourna dans le soir, en souvenir de ses boutiques de sa femme, et s'en retourna dans le soir, en souvenir de ses boutiques de sa femme, et s'en retourna dans le soir, en souvenir de ses boutiques de sa femme, et s'en retourna dans le soi\"], tensor([-332.1220], device='cuda:0'))\n",
            "-- END OF EPOCH 38.\n",
            "Average loss: 1.0621708631515503.\n",
            "911.6486253738403 s elapsed (i.e. 23.375605778816418 s/epoch)\n",
            "[\"-- D'ailleurs, il ne m'aime plus, pensait-elle; que devenir? quel secours attendre, quelle consolation, quel allégement?\"]\n",
            "([\"-- Ce n'est pas le plus de la contre le curé de la catastrophe. Elle avait le percepteur, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en pleine de cours, et elle se penchait en\"], tensor([-298.9541], device='cuda:0'))\n",
            "-- END OF EPOCH 39.\n",
            "Average loss: 1.0583471972891625.\n",
            "935.0221486091614 s elapsed (i.e. 23.375553715229035 s/epoch)\n",
            "[\"Le jeune homme en prit un. C'était la première fois qu'il achetait des fleurs pour une femme; et sa poitrine, en les respirant, se gonfla d'orgueil, comme si cet hommage qu'il destinait à une autre se fût retourné vers lui.\"]\n",
            "([\"-- Ah! c'est un mari, je ne sais pas le conseiller de la considération et de sa pensée, s'était remua dans la salle, avec les premières considérations de sa poitrine, le conseiller de la cheminée, un peu de couleur avait envie de se rapprochement, et les premières chantes de la boutique, le prés de la cheminée, un peu de couleur avait envie de se rapprochement, et les premières chantes de la boutique, le prés de la cheminée, un peu de couleur avait envie de se rapprochement, et les premières chantes de la b\"], tensor([-320.5031], device='cuda:0'))\n",
            "-- END OF EPOCH 40.\n",
            "Average loss: 1.0466451873170568.\n",
            "958.6347024440765 s elapsed (i.e. 23.381334205953085 s/epoch)\n",
            "['-- Ma fille! Emma! mon enfant! expliquez-moi...?']\n",
            "(['-- Ah! pas de la compagne!'], tensor([-17.0887], device='cuda:0'))\n",
            "-- END OF EPOCH 41.\n",
            "Average loss: 1.0313454477683357.\n",
            "980.9616765975952 s elapsed (i.e. 23.356230395180837 s/epoch)\n",
            "[\"-- Oh! ce n'est pas la peine! reprit Lheureux.\"]\n",
            "(['-- Comme il est de la conversionnable sentimentalité.'], tensor([-32.3117], device='cuda:0'))\n",
            "-- END OF EPOCH 42.\n",
            "Average loss: 1.0293197771336169.\n",
            "1003.7993173599243 s elapsed (i.e. 23.344170171161032 s/epoch)\n",
            "[\"Enfin on entendit un choc; les cordes en grinçant remontèrent. Alors Bournisien prit la bêche que lui tendait Lestiboudois; de sa main gauche, tout en aspergeant de la droite, il poussa vigoureusement une large pelletée; et le bois du cercueil, heurté par les cailloux, fit ce bruit formidable qui nous semble être le retentissement de l'éternité.\"]\n",
            "([\"-- Oh! c'est possée!\"], tensor([-12.1698], device='cuda:0'))\n",
            "-- END OF EPOCH 43.\n",
            "Average loss: 1.022369014455917.\n",
            "1026.335696697235 s elapsed (i.e. 23.325811288573526 s/epoch)\n",
            "['-- Oh! non, là-bas, chez nous.']\n",
            "([\"-- Cela vous semait décourage. Il n'avait pas de ce qui la retire dans les chevaux de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la chaise et la prenait des cheveux du pays de la \"], tensor([-346.7098], device='cuda:0'))\n",
            "-- END OF EPOCH 44.\n",
            "Average loss: 1.0158363783613165.\n",
            "1049.8032619953156 s elapsed (i.e. 23.32896137767368 s/epoch)\n",
            "[\"Il leur fallait un bon quart d'heure pour les adieux. Alors Emma pleurait; elle aurait voulu ne jamais abandonner Rodolphe. Quelque chose de plus fort qu'elle la poussait vers lui, si bien qu'un jour, la voyant survenir à l'improviste, il fronça le visage comme quelqu'un de contrarié.\"]\n",
            "([\"-- C'est vrai, dit le pharmacien, par le professeur.\"], tensor([-27.3450], device='cuda:0'))\n",
            "-- END OF EPOCH 45.\n",
            "Average loss: 1.022417223971823.\n",
            "1072.9280030727386 s elapsed (i.e. 23.324521805929102 s/epoch)\n",
            "['-- Mais..., cependant..., raisonnons.']\n",
            "([\"-- Ah! c'est trop faute? c'est le commerce d'une voix manger. Elle ne se perdait pas quelque chose de suivre en place dans les chambres à la file des paroles de la chambre, et le chargeait des paroles de la chambre, en effet, le soir, au coude des larmes se tenaient en parlant, et le conseillait des les arts, des arbres de la chambre, en effet, le soir, au coude des larmes se tenaient en parlant, et le conseillait des les arts, des arbres de la chambre, en effet, le soir, au coude des larmes se tenaient en \"], tensor([-334.9123], device='cuda:0'))\n",
            "-- END OF EPOCH 46.\n",
            "Average loss: 1.007378304258306.\n",
            "1096.7313601970673 s elapsed (i.e. 23.334709791426963 s/epoch)\n",
            "[\"-- Mais, dit le médecin, j'ai peur pour lui que... là-bas...\"]\n",
            "(['-- Ah! mon pauvre femme! dit le pharmacien, qui ne serait pas de resser par le bout.'], tensor([-50.9769], device='cuda:0'))\n",
            "-- END OF EPOCH 47.\n",
            "Average loss: 1.0060030564348748.\n",
            "1120.3283598423004 s elapsed (i.e. 23.34017416338126 s/epoch)\n",
            "[\"Le notaire entra, serrant du bras gauche contre son corps sa robe de chambre à palmes, tandis qu'il ôtait et remettait vite de l'autre main sa toque de velours marron, prétentieusement posée sur le côté droit, où retombaient les bouts de trois mèches blondes qui, prises à l'occiput, contournaient son crâne chauve.\"]\n",
            "([\"-- C'est qu'il n'y avait pas la contemplation. Elle se répétait d'une porte entre les pareils de la cheminée. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la chambre. Elle se répétait dans la cha\"], tensor([-263.7066], device='cuda:0'))\n",
            "-- END OF EPOCH 48.\n",
            "Average loss: 0.9988255361293225.\n",
            "1144.617815732956 s elapsed (i.e. 23.359547259856242 s/epoch)\n",
            "[\"-- Après tout, c'est vrai, pensa Rodolphe; j'agis dans son intérêt; je suis honnête.\"]\n",
            "([\"-- Ce n'est pas le professeur, dit-il en la rencontrant de la poitrine.\"], tensor([-40.8676], device='cuda:0'))\n",
            "-- END OF EPOCH 49.\n",
            "Average loss: 0.9950654189637367.\n",
            "1168.0431983470917 s elapsed (i.e. 23.360863966941835 s/epoch)\n",
            "[\"-- Il est vrai, répondit Emma; mais le dérangement m'amuse toujours; j'aime à changer de place.\"]\n",
            "([\"-- Oui, j'ai tort! je suis soude des choses pour se distraire, il n'y avait pas la contemplation de ses petites fantailles de son cheval, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les préparatifs, elle se penchait en plus commode pour les pr\"], tensor([-274.9874], device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxs4UZRAHxx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facc1f25-b834-40d2-8ffe-fa454541e88e"
      },
      "source": [
        "with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "  if(use_toy_dataset):\n",
        "    prompt = [\"AAAA. Now, please write 3 i.\" + EOP] #[paragraphs[0]]\n",
        "    print(prompt[0])\n",
        "    print(f\"(Is this prompt in the training set? {prompt[0] in paragraphs})\\n\")\n",
        "\n",
        "    for _ in range(10):\n",
        "      batch = batch_generator.turn_into_batch(prompt)\n",
        "      gen_texts, logprobs = model.predictionStrings(batch.to(model.device), max_predicted_char=128)\n",
        "\n",
        "      print(f\"{gen_texts[0]} (log-probability: {logprobs.item()})\")\n",
        "      prompt = [gen_texts[0] + EOP]\n",
        "  else:\n",
        "    prompt = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"] * 10\n",
        "    batch = batch_generator.turn_into_batch(prompt)\n",
        "    gen_texts, logprobs = model.predictionStrings(batch.to(model.device), max_predicted_char=1024)\n",
        "\n",
        "    print(prompt[0])\n",
        "    print()\n",
        "    for i, (gen_text, logprob) in enumerate(zip(gen_texts, logprobs)):\n",
        "      print(f\"{i}: \", end=\"\")\n",
        "      print(gen_text)\n",
        "      print(f\"log-probability: {logprob.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\n",
            "\n",
            "0: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "1: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "2: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "3: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "4: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "5: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "6: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "7: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "8: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n",
            "9: -- Ah! c'est vous compte un chapeau.\n",
            "log-probability: -20.58887481689453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, len(average_losses) + 1))  # Epoch numbers\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, average_losses, label='Average Loss', marker='o', linestyle='-')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Average Loss Per Epoch')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Y6ELZ4mEZ7QM",
        "outputId": "7af1e940-3196-4326-8d9a-993e9e839e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}